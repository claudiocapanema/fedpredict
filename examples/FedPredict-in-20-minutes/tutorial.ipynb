{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flower + Pytorch + FedPredict\n",
    "\n",
    "This tutorial implements a simple example of \"Flower + Pytorch + FedPredict\".\n",
    "For comparison, it it possible to execute the strategies \"FedAvg+FP\" (i.e., using FedPredict) and \"FedAvg\" (i.e., original solution).\n",
    "\n",
    "To run in a Jupyer or [![Google Colab environment](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/claudiocapanema/fedpredict/examples/FedPredict-in-20-minutes/tutorial.ipynb), this example uses the method `run_simulation` from:\n",
    "```Python\n",
    "from flwr.simulation import run_simulation\n",
    "```\n",
    "\n",
    "However, using the `flwr run` CLI command to launch experiments it more professional. A similar tutorial using this command can be found in [here]()."
   ],
   "id": "c1ef3edc193f1174"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment config",
   "id": "3d0d3220c1a4353f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:36:30.063331Z",
     "start_time": "2025-05-03T00:36:30.059853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 32\n",
    "ALPHA = 0.1 # [0.1, 1.0]\n",
    "STRATEGY = \"FedAvg+FP\" # FedAvg+FP or FedAvg\n",
    "LOCAL_EPOCHS = 1\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_ROUNDS = 10\n",
    "NUM_PARTITIONS = 10 # number of clients"
   ],
   "id": "546720e8cc3c467d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model definition and utils",
   "id": "4bb93fe2ce4530cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:36:30.122991Z",
     "start_time": "2025-05-03T00:36:30.112390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"pytorch_fedpredict_example: A Flower / PyTorch app.\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # Configure logging\n",
    "logger = logging.getLogger(__name__)  # Create logger for the module\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Model (simple CNN adapted from 'PyTorch: A 60 Minute Blitz')\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def get_weights(net):\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_weights(net, parameters):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "fds = None  # Cache FederatedDataset\n",
    "\n",
    "\n",
    "def load_data(partition_id: int, num_partitions: int, alpha: float, batch_size: int):\n",
    "    \"\"\"Load partition CIFAR10 data.\"\"\"\n",
    "    # Only initialize `FederatedDataset` once\n",
    "    global fds\n",
    "    if fds is None:\n",
    "        partitioner = DirichletPartitioner(num_partitions=num_partitions, partition_by=\"label\",\n",
    "                                     alpha=alpha, min_partition_size=10)\n",
    "        fds = FederatedDataset(\n",
    "            dataset=\"uoft-cs/cifar10\",\n",
    "            partitioners={\"train\": partitioner},\n",
    "        )\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    pytorch_transforms = Compose(\n",
    "        [ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_test[\"train\"], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    testloader = DataLoader(partition_train_test[\"test\"], batch_size=batch_size)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def train(net, trainloader, valloader, epochs, learning_rate, device):\n",
    "    \"\"\"Train the model on the training set.\"\"\"\n",
    "    net.to(device)  # move model to GPU if available\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        for batch in trainloader:\n",
    "            images = batch[\"img\"]\n",
    "            labels = batch[\"label\"]\n",
    "            optimizer.zero_grad()\n",
    "            criterion(net(images.to(device)), labels.to(device)).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    val_loss, val_acc = test(net, valloader, device)\n",
    "\n",
    "    results = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_acc,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def test(net, testloader, device):\n",
    "    \"\"\"Validate the model on the test set.\"\"\"\n",
    "    net.to(device)  # move model to GPU if available\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    loss = loss / len(testloader)\n",
    "    return loss, accuracy"
   ],
   "id": "ddfcdfe9fa995882",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FedAvg client",
   "id": "bf9fcd9ebf40e2cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:36:30.170395Z",
     "start_time": "2025-05-03T00:36:30.161548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"pytorch_fedpredict_example: A Flower / PyTorch / FedPredict app.\"\"\"\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "import torch\n",
    "from flwr.client import NumPyClient\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # Configure logging\n",
    "logger = logging.getLogger(__name__)  # Create logger for the module\n",
    "\n",
    "# Define FedAvg Client\n",
    "class FedAvgClient(NumPyClient):\n",
    "    def __init__(self, trainloader, valloader, local_epochs, learning_rate, num_server_rounds, client_id, client_state):\n",
    "        try:\n",
    "            self.local_model = Net()\n",
    "            self.trainloader = trainloader\n",
    "            self.valloader = valloader\n",
    "            self.local_epochs = local_epochs\n",
    "            self.lr = learning_rate\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"__init__ error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train the model with data of this client.\"\"\"\n",
    "        try:\n",
    "            \"\"\"Train the model with data of this client.\"\"\"\n",
    "            set_weights(self.local_model, parameters)\n",
    "            results = train(\n",
    "                self.local_model,\n",
    "                self.trainloader,\n",
    "                self.valloader,\n",
    "                self.local_epochs,\n",
    "                self.lr,\n",
    "                self.device,\n",
    "            )\n",
    "            return get_weights(self.local_model), len(self.trainloader.dataset), results\n",
    "        except Exception as e:\n",
    "            logger.error(\"fit error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate the model on the data this client has.\"\"\"\n",
    "        try:\n",
    "            set_weights(self.local_model, parameters)\n",
    "            loss, accuracy = test(self.local_model, self.valloader, self.device)\n",
    "            return loss, len(self.valloader.dataset), {\"accuracy\": accuracy}\n",
    "        except Exception as e:\n",
    "            logger.error(\"evaluate error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))"
   ],
   "id": "4c482629c994aadb",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### FedAvg+FP client\n",
    "\n",
    "This class implements the required modifications to include the plugin."
   ],
   "id": "ffcb160d32320a54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:36:30.218560Z",
     "start_time": "2025-05-03T00:36:30.209751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"pytorch_fedpredict_example: A Flower / PyTorch / FedPredict app.\"\"\"\n",
    "\n",
    "import copy\n",
    "from fedpredict import fedpredict_client_torch\n",
    "from flwr.common import ConfigRecord\n",
    "\n",
    "# Define FedAvg+FP Client\n",
    "class FedAvgClientFP(FedAvgClient):\n",
    "    def __init__(self, trainloader, valloader, local_epochs, learning_rate, num_server_rounds, client_id, client_state):\n",
    "        try:\n",
    "            super().__init__(trainloader, valloader, local_epochs, learning_rate, num_server_rounds, client_id, client_state)\n",
    "            self.global_model = copy.deepcopy(self.local_model)\n",
    "            self.lt = 0 # last round the client trained\n",
    "            self.num_server_rounds = num_server_rounds\n",
    "            self.client_state = client_state\n",
    "        except Exception as e:\n",
    "            logger.error(\"__init__ error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train the model with data of this client.\"\"\"\n",
    "        try:\n",
    "            t = config[\"server_round\"]\n",
    "            self.lt = t\n",
    "            results = super().fit(parameters, config)\n",
    "            self._save_layer_weights_to_state()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(\"fit error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate the model on the data this client has.\"\"\"\n",
    "        try:\n",
    "\n",
    "            set_weights(self.global_model, parameters)\n",
    "            t = config[\"server_round\"]\n",
    "            self._load_layer_weights_from_state()\n",
    "            # Calculate the number of consecutive rounds the client has not been selected for training (nt).\"\n",
    "            nt = t - self.lt\n",
    "            # Get the \"combined_model\" from FedPredict\n",
    "            combined_model = fedpredict_client_torch(local_model=self.local_model, global_model=self.global_model,\n",
    "                                                     t=t, T=self.num_server_rounds, nt=nt, device=self.device)\n",
    "            # Test the \"combined_model\"\n",
    "            loss, accuracy = test(combined_model, self.valloader, self.device)\n",
    "            return loss, len(self.valloader.dataset), {\"accuracy\": accuracy}\n",
    "        except Exception as e:\n",
    "            logger.error(\"evaluate error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "    def _save_layer_weights_to_state(self):\n",
    "        \"\"\"Save last layer weights to state.\"\"\"\n",
    "        try:\n",
    "            arr_record = ArrayRecord(torch_state_dict=self.local_model.state_dict())\n",
    "\n",
    "            # Add to RecordDict (replace if already exists)\n",
    "            self.client_state[\"model\"] = arr_record\n",
    "            self.client_state[\"lt\"] = ConfigRecord(config_dict={\"lt\": self.lt})\n",
    "        except Exception as e:\n",
    "            logger.error(\"_save_layer_weights_to_state error\")\n",
    "            logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "    def _load_layer_weights_from_state(self):\n",
    "        \"\"\"Load last layer weights to state.\"\"\"\n",
    "        if \"model\" not in self.client_state.array_records:\n",
    "            return\n",
    "\n",
    "        state_dict = self.client_state[\"model\"].to_torch_state_dict()\n",
    "        self.lt = self.client_state[\"lt\"][\"lt\"]\n",
    "\n",
    "        # apply previously saved classification head by this client\n",
    "        self.local_model.load_state_dict(state_dict, strict=True)"
   ],
   "id": "821ca0983c06d052",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Client App\n",
    "\n",
    "Here we define the client app:"
   ],
   "id": "f35a639f77649197"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:36:30.270143Z",
     "start_time": "2025-05-03T00:36:30.265404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"pytorch_fedpredict_example: A Flower / PyTorch / FedPredict app.\"\"\"\n",
    "\n",
    "import sys\n",
    "from flwr.client import ClientApp\n",
    "from flwr.common import ArrayRecord, Context\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # Configure logging\n",
    "logger = logging.getLogger(__name__)  # Create logger for the module\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    \"\"\"Construct a Client that will be run in a ClientApp.\"\"\"\n",
    "    try:\n",
    "        # Read the node_config to fetch data partition associated to this node\n",
    "        partition_id = context.node_config[\"partition-id\"]\n",
    "        trainloader, valloader = load_data(partition_id, num_partitions, ALPHA, BATCH_SIZE)\n",
    "        client_state = context.state\n",
    "\n",
    "        # Return Client instance\n",
    "        if STRATEGY == \"FedAvg+FP\":\n",
    "            return FedAvgClientFP(trainloader, valloader, LOCAL_EPOCHS, LEARNING_RATE, NUM_ROUNDS, partition_id,\n",
    "                                client_state).to_client()\n",
    "        elif STRATEGY == \"FedAvg\":\n",
    "            return FedAvgClient(trainloader, valloader, LOCAL_EPOCHS, LEARNING_RATE, NUM_ROUNDS, partition_id,\n",
    "                                  client_state).to_client()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"client_fn error\")\n",
    "        logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "# Flower ClientApp\n",
    "try:\n",
    "    client_app = ClientApp(client_fn)\n",
    "except Exception as e:\n",
    "    logger.error(\"app error\")\n",
    "    logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n"
   ],
   "id": "fbef1d28447bd2b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Server App\n",
    "\n",
    "Here we define the server app:"
   ],
   "id": "b5ab234571d3d1a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:36:30.318829Z",
     "start_time": "2025-05-03T00:36:30.313116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"pytorch_fedpredict_example: A Flower / PyTorch / FedPredict app.\n",
    "    This basic version of FedPredict requires a small modification\n",
    "    on the server side:the server must communicate the current\n",
    "     training round number to the selected client during each\n",
    "     training cycle. \"\"\"\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from flwr.common import Context, Metrics, Scalar, ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # Configure logging\n",
    "logger = logging.getLogger(__name__)  # Create logger for the module\n",
    "\n",
    "\n",
    "# Define metric aggregation function\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "def on_fit_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    config = {\"server_round\": server_round}\n",
    "    return config\n",
    "\n",
    "def on_evaluate_config_fn(server_round: int) -> dict[str, Scalar]:\n",
    "    config = {\"server_round\": server_round}\n",
    "    return config\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    \"\"\"Construct components that set the ServerApp behaviour.\"\"\"\n",
    "    try:\n",
    "\n",
    "        # Initialize model parameters\n",
    "        ndarrays = get_weights(Net())\n",
    "        parameters = ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "        # Define the strategy\n",
    "        strategy = FedAvg(\n",
    "            fraction_fit=0.3,\n",
    "            fraction_evaluate=1,\n",
    "            min_available_clients=2,\n",
    "            on_fit_config_fn=on_fit_config_fn,\n",
    "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
    "            evaluate_metrics_aggregation_fn=weighted_average,\n",
    "            initial_parameters=parameters,\n",
    "        )\n",
    "        config = ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "\n",
    "        return ServerAppComponents(strategy=strategy, config=config)\n",
    "    except Exception as e:\n",
    "        logger.error(\"server_fn error\")\n",
    "        logger.error(\"\"\"Error on line {} {} {}\"\"\".format(sys.exc_info()[-1].tb_lineno, type(e).__name__, e))\n",
    "\n",
    "# Create ServerApp\n",
    "server_app = ServerApp(server_fn=server_fn)\n"
   ],
   "id": "ba21a090578e54d5",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Launching the Simulation\n",
    "\n",
    "With both `ClientApp` and `ServerApp` ready, we can launch the simulation. Pass both apps to the `run_simulation()` function and specify the number of `supernodes` (this is a more general term used in Flower to refer to individual \"nodes\" or \"clients\"). We earlier partitioned the dataset into 100 partitions, one for each supernode. So we indicate that `num_supernodes`=100."
   ],
   "id": "48a6ae3c588cd62a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T00:38:19.927237Z",
     "start_time": "2025-05-03T00:36:30.362808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flwr.simulation import run_simulation\n",
    "\n",
    "run_simulation(\n",
    "    server_app=server_app, client_app=client_app, num_supernodes=NUM_PARTITIONS\n",
    ")"
   ],
   "id": "a2d208211d041cfb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92mINFO \u001B[0m:      Starting Flower ServerApp, config: num_rounds=10, no round_timeout\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [INIT]\n",
      "\u001B[92mINFO \u001B[0m:      Using initial global parameters provided by strategy\n",
      "\u001B[92mINFO \u001B[0m:      Starting evaluation of initial global parameters\n",
      "\u001B[92mINFO \u001B[0m:      Evaluation returned no results (`None`)\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 1]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[33m(raylet)\u001B[0m bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\u001B[36m(ClientAppActor pid=26420)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "\u001B[36m(ClientAppActor pid=26420)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "\u001B[33m(raylet)\u001B[0m bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\u001B[32m [repeated 5x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[93mWARNING \u001B[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 2]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26420)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 5x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26420)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 5x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 3]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26417)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 10x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26417)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 10x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 4]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 10x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 10x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 5]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26416)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\n",
      "\u001B[36m(ClientAppActor pid=26416)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26420)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 12x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26420)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 12x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 6]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 7]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 8]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 9]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [ROUND 10]\n",
      "\u001B[92mINFO \u001B[0m:      configure_fit: strategy sampled 3 clients (out of 10)\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_fit: received 3 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      configure_evaluate: strategy sampled 10 clients (out of 10)\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26419)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26415)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 13x across cluster]\u001B[0m\n",
      "\u001B[92mINFO \u001B[0m:      aggregate_evaluate: received 10 results and 0 failures\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[92mINFO \u001B[0m:      [SUMMARY]\n",
      "\u001B[92mINFO \u001B[0m:      Run finished 10 round(s) in 107.23s\n",
      "\u001B[92mINFO \u001B[0m:      \tHistory (loss, distributed):\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 1: 1.7234716126860194\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 2: 1.4179249935915734\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 3: 1.3739353155588203\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 4: 1.2287678949459395\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 5: 1.1991021016260284\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 6: 1.2034634213979352\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 7: 1.1012931279685876\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 8: 1.0214641455864018\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 9: 0.9885832093587917\n",
      "\u001B[92mINFO \u001B[0m:      \t\tround 10: 0.9814403209204512\n",
      "\u001B[92mINFO \u001B[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001B[92mINFO \u001B[0m:      \t{'accuracy': [(1, 0.3282686925229908),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (2, 0.4855057976809276),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (3, 0.5078968412634945),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (4, 0.5911635345861656),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (5, 0.5583766493402639),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (6, 0.5783686525389844),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (7, 0.6083566573370651),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (8, 0.6416433426629349),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (9, 0.6473410635745702),\n",
      "\u001B[92mINFO \u001B[0m:      \t              (10, 0.6566373450619752)]}\n",
      "\u001B[92mINFO \u001B[0m:      \n",
      "\u001B[36m(ClientAppActor pid=26418)\u001B[0m /home/gustavo/Documentos/virtual_environments/FL-HIAAC_docker/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\u001B[32m [repeated 10x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26418)\u001B[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\u001B[32m [repeated 10x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26418)\u001B[0m Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub\u001B[32m [repeated 9x across cluster]\u001B[0m\n",
      "\u001B[36m(ClientAppActor pid=26418)\u001B[0m Found the latest cached dataset configuration 'plain_text' at /home/gustavo/.cache/huggingface/datasets/uoft-cs___cifar10/plain_text/0.0.0/0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Thu May  1 10:36:05 2025).\u001B[32m [repeated 9x across cluster]\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
